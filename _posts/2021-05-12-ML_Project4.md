---
layout: post
title:  "ML - 2014210150 박성호 4차 과제"
---

# 머신러닝 4차 과제

# 학번 : 2014210150
# 이름 : 박성호

# 과제 1 : 로지스틱 회귀 구현 (1)
 * 초기 설정 및 데이터 불러오기
 * 로지스틱 회귀 이진 분류를 위해 버지니카 품종과 버지니카 품종이 아닌 것을 분류하기 위한 설정
 * 데이터셋 분할
 * 타깃 변환

## **기본 설정**


```python
# 파이썬 >= 3.5
import sys
assert sys.version_info >= (3, 5)

# 사이킷런 >= 0.2
import sklearn
assert sklearn.__version__ >= "0.20"

# numpy 모듈
import numpy as np
import os

# 그래프 출력을 위한 모듈
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt

# 노트북 실행 결과 유지
np.random.seed(30)
```

## **데이터 불러오기**
* 붓꽃 데이터셋을 불러온 후 꽃잎 길이와 꽃잎 너비 특성만을 추출한다.


```python
from sklearn import datasets
iris = datasets.load_iris() #붓꽃 데이터셋 불러오기
```


```python
iris.keys() #붓꽃 데이터셋의 키 확인
```




    dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])




```python
x = iris["data"][:, (2, 3)] #붓꽃 데이터의 꽃잎 길이, 꽃잎 너비 특성 추출
y = (iris["target"] == 2).astype(np.int) # 이진 분류를 위한 품종 나누기
```


```python
len(x) # 붓꽃 데이터의 개수
```




    150




```python
x[0:10] # [꽃잎 길이, 꽃잎 너비]
```




    array([[1.4, 0.2],
           [1.4, 0.2],
           [1.3, 0.2],
           [1.5, 0.2],
           [1.4, 0.2],
           [1.7, 0.4],
           [1.4, 0.3],
           [1.5, 0.2],
           [1.4, 0.2],
           [1.5, 0.1]])



모든 샘플에 편향을 추가하여 0번 특성값이 항상 1이라 가정


```python
x_with_bias = np.c_[np.ones([len(x), 1]), x]
x_with_bias[0:10]
```




    array([[1. , 1.4, 0.2],
           [1. , 1.4, 0.2],
           [1. , 1.3, 0.2],
           [1. , 1.5, 0.2],
           [1. , 1.4, 0.2],
           [1. , 1.7, 0.4],
           [1. , 1.4, 0.3],
           [1. , 1.5, 0.2],
           [1. , 1.4, 0.2],
           [1. , 1.5, 0.1]])



타깃의 개수 = 150

0 : 버지니카가 아닌 붓꽃 (100개)

1 : 버지니카 (50개)



```python
len(y) # 타깃의 개수
```




    150




```python
y # 버지니카 품종이 아닌 붓꽃(0), 버지니카(1)
```




    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])



## **데이터셋 분할**
* 데이터셋을 훈련, 검증, 테스트 용도로 일정 비율로 무작위 분할한다.

 * 훈련 세트 : 60%
 * 검증 세트 : 20%
 * 테스트 세트 : 20%


```python
valid_ratio = 0.2 # 검증 세트 20%
test_ratio = 0.2 # 테스트 세트 20%
total_size = len(x_with_bias) # 전체 데이터셋

valid_size = int(total_size * valid_ratio) # 검증 세트 크기 : 전체의 20%
test_size = int(total_size * test_ratio) # 테스트 세트 크기 : 전체의 20%
train_size = total_size - (test_size + valid_size) # 훈련 세트 크기 : 전체의 60%
```


```python
print(train_size) # 훈련 세트 크기
print(valid_size) # 검증 세트 크기
print(test_size) # 테스트 세트 크기
```

    90
    30
    30


**np.random.permutation()** 함수를 이용하여 인덱스를 무작위로 섞는다.



```python
rd_index = np.random.permutation(total_size)
```

무작위로 섞인 인덱스를 사용하여 6:2:2 비율로 훈련, 검증, 테스트 세트를 분할한다.


```python
x_train = x_with_bias[rd_index[:train_size]]
y_train = y[rd_index[:train_size]]

x_valid = x_with_bias[rd_index[train_size:-test_size]]
y_valid = y[rd_index[train_size:-test_size]]

x_test = x_with_bias[rd_index[-test_size:]]
y_test = y[rd_index[-test_size:]]
```

무작위로 섞여 알맞는 비율로 분할되었는지 확인


```python
x_train[:5]
```




    array([[1. , 1.7, 0.2],
           [1. , 1.7, 0.4],
           [1. , 1.5, 0.2],
           [1. , 5.1, 1.9],
           [1. , 3.9, 1.2]])




```python
x_valid[:5]
```




    array([[1. , 4.2, 1.5],
           [1. , 5.1, 2.4],
           [1. , 4.9, 1.5],
           [1. , 1.4, 0.2],
           [1. , 1.6, 0.2]])




```python
x_test[:5]
```




    array([[1. , 4.5, 1.5],
           [1. , 4.6, 1.4],
           [1. , 4.6, 1.3],
           [1. , 5. , 2. ],
           [1. , 1.4, 0.2]])




```python
# 6:2:2 비율의 무작위로 분활된 훈련, 검증, 테스트 세트
print(len(x_train))
print(len(x_valid))
print(len(x_test))
```

    90
    30
    30


## **타깃 변환**
* 0, 1로 설정되어 있는 버지니카가 아닌 품종과 버지니카 품종을 원-핫 벡터로 표현한다.
* 0과 1의 수치적 표현으로 인해 원치 않는 비중을 막기 위함


```python
def one_hot_vector(y):
    n = y.max() + 1 # 클래스 수(2)
    m = len(y) # 샘플 수(150)
    y_one_hot = np.zeros((m, n)) # (150, 2) 크기의 0 벡터 생성
    y_one_hot[np.arange(m), y] = 1 # 해당 클래스 1 값 삽입
    return y_one_hot
```

원-핫 벡터 함수가 잘 작동하는지 확인


```python
y_train[:5]
```




    array([0, 0, 0, 1, 0])




```python
one_hot_vector(y_train[:5])
```




    array([[1., 0.],
           [1., 0.],
           [1., 0.],
           [0., 1.],
           [1., 0.]])



훈련, 검증, 테스트 세트의 타깃을 모두 원-핫 벡터로 변환


```python
y_train_one_hot = one_hot_vector(y_train)
y_valid_one_hot = one_hot_vector(y_valid)
y_test_one_hot = one_hot_vector(y_test)
```

#과제 1 로지스틱 회귀 구현 (2)
* 로지스틱 회귀를 구현하기 위해
 1.   시그모이드 함수를 통한 예측
 2.   경사 하강법 적용
 3.   규제가 사용된 경사하강법 활용
 4.   조기종료를 추가한 경사하강법 활용



## **1. 시그모이드 함수 구현**


```python
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```

임의의 배열을 생성하여 시그모이드 함수에 적용한 모습


```python
ex = np.arange(-5, 6, 1) # [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]
sig_ex = sigmoid(ex) # 위 배열을 시그모이드 함수에 적용

#그래프 그리기
plt.plot(ex, sig_ex)
plt.ylim(0, 1) # y축 범위
plt.axvline(0, color='black') # x = 0인 세로 막대
plt.xlabel('ex') # x 라벨
plt.ylabel('sigmoid_ex') # y 라벨
plt.yticks([0, 0.5, 1.0]) # y축 눈금 표시
ax = plt.gca()
ax.yaxis.grid(True)

plt.show()
```

![output_38_0](https://user-images.githubusercontent.com/80394894/117976245-84273780-b36a-11eb-86ee-2f5a366343fb.png)

클래스별 파라미터로 이루어진 행렬과 2차원 넘파이 어레이 


```python
n_inputs = x_train.shape[1] # 특성 수 + 1 : 3개
n_outputs = len(np.unique(y_train)) # 클래스 수 : 2개 (버지니카 or 버지니카가 아닌 품종)
```


```python
Theta = np.random.randn(n_inputs, n_outputs)
```

## **경사 하강법 적용**
 * 시그모이드 함수 적용
 * 비용함수 (손실 로그 기법)
 ![loss.PNG](data:image/png;base64,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)

 loss 변수에 수식 적용

배치 경사하강법 훈련 코드

- `eta = 0.01`: 학습률
- `n_iterations = 5001` : 에포크 수
- `m = len(X_train)` : 훈련 세트 크기, 즉 훈련 샘플 수
- `epsilon = 1e-7` : $\log$ 값이 항상 계산되도록 더해지는 작은 실수
- `logits` : 모든 샘플에 대한 클래스별 점수, 즉 $\mathbf{X}_{\textit{train}}\, \Theta$
- `y_proba` : 모든 샘플에 대해 계산된 클래스 별 소속 확률, 즉 $\hat P$


```python
eta = 0.01
n_iterations = 5001
m = len(x_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = x_train.dot(Theta)
    y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        # 비용 함수 = 로그 손실 함수
        loss = -1/m * (np.sum(y_train_one_hot * np.log(y_proba + epsilon) + (1 - y_train_one_hot) * np.log((1 - y_proba) + epsilon)))
        print(iteration, loss)
    
    error = y_proba - y_train_one_hot     # 그레이디언트 계산.
    gradients = 1/m * x_train.T.dot(error)
    
    Theta = Theta - (eta * gradients)       # 파라미터 업데이트
```

    0 4.826498891898852
    500 1.4514180977498639
    1000 1.202091200086981
    1500 1.0412847495860127
    2000 0.9324398548890692
    2500 0.8545627321865802
    3000 0.7960335593663244
    3500 0.7502427052141599
    4000 0.7132495897702164
    4500 0.6825871361990947
    5000 0.6566398956775238


학습된 파라미터


```python
Theta
```




    array([[ 3.34997923e+00, -3.65888765e+00],
           [ 8.23067638e-04,  4.83497245e-01],
           [-2.04599475e+00,  8.07878969e-01]])



검증 세트에 대한 예측과 정확도로 y_proba에서 가장 큰 값을 갖는 인덱스로 선택


```python
logits = x_valid.dot(Theta)              
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)          # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    1.0



## **규제가 추가된 경사하강범 활용**
학습률 `eta` 증가

`alpha = 0.1` : 규제 강도


```python
eta = 0.1
n_iterations = 5001
m = len(x_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = x_train.dot(Theta)
    y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:
        # 비용 함수 = 로그 손실 함수
        xentropy_loss = -1/m * (np.sum(y_train_one_hot * np.log(y_proba + epsilon) + (1 - y_train_one_hot) * np.log((1 - y_proba) + epsilon)))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = y_proba - y_train_one_hot
    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * x_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```

    0 2.902309706879797
    500 0.7754779752624889
    1000 0.7146032555100352
    1500 0.6971936740849871
    2000 0.6908366096164498
    2500 0.6882636761761001
    3000 0.6871652669156789
    3500 0.6866817129927009
    4000 0.6864647943677514
    4500 0.6863663157576795
    5000 0.6863212583271859


l2 패널티를 추가 하였으나 손실이 감소되었고 검증 세트에 대한 정확도는 0.967로 낮아졌다.


```python
logits = x_valid.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.9666666666666667



## **조기 종료 추가**

규제가 사용된 모델의 훈련 과정에서 에포크마다 검증 세트에 대한 손실을 계산하고 오차가 줄어들다가 증가하기 시작할 때 종료


```python
eta = 0.1 
n_iterations = 5001
m = len(x_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = x_train.dot(Theta)
    y_proba = sigmoid(logits)
    error = y_proba - y_train_one_hot
    gradients = 1/m * x_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산(로그 손실 함수)
    logits = x_valid.dot(Theta)
    y_proba = sigmoid(logits)
    xentropy_loss = -1/m * (np.sum(y_valid_one_hot * np.log(y_proba + epsilon) + (1 - y_valid_one_hot) * np.log((1 - y_proba) + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 1.485010041543432
    500 0.2718335650803382
    722 0.2669583087585441
    723 0.2669583794983213 조기 종료!


훈련이 조기 종료된 검증 세트에 대한 정확도는 0.967로 규제를 사용한 결과와 같다.


```python
logits = x_valid.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.9666666666666667



전체 데이터셋에 대한 모델의 예측 그래프

로지스틱 회귀분석은 이진 분류로 버지니카품종과 버지니카가 아닌 품종으로 그래프를 나타낸다.


```python
# (0, 8) x (0, 3.5) 크기의 직사각형 안의 모든 점을 대상으로 예측한 후에
# 예측 결과를 이용하여 색상으로 구분하고 등고선도 그리기 위한 준비작업
# 가로는 500개의 구간으로, 세로는 200개의 구간으로 쪼개짐.
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]
X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]

# 직사각형 점 대상 예측하기
logits = X_new_with_bias.dot(Theta)
Y_proba = sigmoid(logits)
y_predict = np.argmax(Y_proba, axis=1)

# 등고선용 정보
zz1 = Y_proba[:, 1].reshape(x0.shape)                            # 버지니카 기준 예측 확률
zz = y_predict.reshape(x0.shape)                                 # 예측값

# 붓꽃 샘플 그리기
plt.figure(figsize=(10, 4))
plt.plot(x[y==1, 0], x[y==1, 1], "bs", label="Iris virginica")  # 파랑 사각형, 버지니카
plt.plot(x[y==0, 0], x[y==0, 1], "yo", label="Is not virginica") # 노랑 원, 버지니카가 아닌 품종

# 등고선 그리기
from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)                       # 노랑, 청보라 바탕색
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)              # 버지니카 기준 예측 확률 등고선
plt.clabel(contour, inline=1, fontsize=12)

# 기타 도표 정보
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
plt.show()
```

![output_58_0](https://user-images.githubusercontent.com/80394894/117976406-b6389980-b36a-11eb-914e-4bca088b1088.png)



**테스트 세트 평가**

테스트 세트에 대한 모델의 최종 성능의 정확도는 0.967로 나타났다.


```python
logits = x_test.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.9666666666666667



#과제 2 로지스틱 회귀 분류 일대다(OvR) 방식 적용 (1)
 * 다중 클래스 분류를 위한 데이터 설정
 * 데이터셋 분할
 * 타깃 변환

## **다중 클래스 분류를 위한 데이터 설정**


```python
x = iris["data"][:, (2, 3)] #붓꽃 데이터의 꽃잎 길이, 꽃잎 너비 특성 추출
y = iris["target"] # 다중 클래스 분류를 위한 데이터 설정
```

모든 샘플에 편향을 추가하여 0번 특성값이 항상 1이라 가정


```python
x_with_bias = np.c_[np.ones([len(x), 1]), x]
x_with_bias[0:10]
```




    array([[1. , 1.4, 0.2],
           [1. , 1.4, 0.2],
           [1. , 1.3, 0.2],
           [1. , 1.5, 0.2],
           [1. , 1.4, 0.2],
           [1. , 1.7, 0.4],
           [1. , 1.4, 0.3],
           [1. , 1.5, 0.2],
           [1. , 1.4, 0.2],
           [1. , 1.5, 0.1]])



타깃의 개수 = 150

0 : 세토사 (50개)

1 : 버시컬러 (50개)

2 : 버지니카 (50개)


```python
y
```




    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])



## **데이터셋 분할**


```python
valid_ratio = 0.2 # 검증 세트 20%
test_ratio = 0.2 # 테스트 세트 20%
total_size = len(x_with_bias) # 전체 데이터셋

valid_size = int(total_size * valid_ratio) # 검증 세트 크기 : 전체의 20%
test_size = int(total_size * test_ratio) # 테스트 세트 크기 : 전체의 20%
train_size = total_size - (test_size + valid_size) # 훈련 세트 크기 : 전체의 60%
```

무작위 분할 설정


```python
rd_index = np.random.permutation(total_size)

x_train = x_with_bias[rd_index[:train_size]]
y_train = y[rd_index[:train_size]]

x_valid = x_with_bias[rd_index[train_size:-test_size]]
y_valid = y[rd_index[train_size:-test_size]]

x_test = x_with_bias[rd_index[-test_size:]]
y_test = y[rd_index[-test_size:]]
```

## **타깃 변환**
* 0, 1, 2로 설정되어 있는 세토사, 버시컬러, 버지니카 품종을 원-핫 벡터로 표현한다.
* 수치적 표현으로 인해 원치 않는 비중을 막기 위함


```python
y_train_one_hot = one_hot_vector(y_train)
y_valid_one_hot = one_hot_vector(y_valid)
y_test_one_hot = one_hot_vector(y_test)
```

#과제 2 로지스틱 회귀 분류 일대다(OvR) 방식 적용 (2)
 1.   경사 하강법 활용
 2.   규제가 사용된 경사하강법 활용
 3.   조기종료를 추가한 경사하강법 활용


```python
n_inputs = x_train.shape[1] # 특성 수 + 1 = 3
n_outputs = len(np.unique(y_train)) # 클래스 수 = 3
```


```python
Theta = np.random.randn(n_inputs, n_outputs)
```

## **1. 배치 경사 하강법 적용**


```python
eta = 0.01
n_iterations = 5001
m = len(x_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = x_train.dot(Theta)
    y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -1/m * (np.sum(y_train_one_hot * np.log(y_proba + epsilon) + (1 - y_train_one_hot) * np.log((1 - y_proba) + epsilon)))
        print(iteration, loss)
    
    error = y_proba - y_train_one_hot     # 그레이디언트 계산.
    gradients = 1/m * x_train.T.dot(error)
    
    Theta = Theta - eta * gradients       # 파라미터 업데이트
```

    0 10.282320483937543
    500 1.6550137888728191
    1000 1.4820598862950296
    1500 1.360134041629436
    2000 1.2718561713912322
    2500 1.2058746259270672
    3000 1.154984541743466
    3500 1.114603710792644
    4000 1.0817630286202695
    4500 1.0544883853829432
    5000 1.0314304303358126


경사 하강법을 활용한 검증 세트에 대한 예측과 정확도는 0.867


```python
logits = x_valid.dot(Theta)              
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)          # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```




    0.8666666666666667



## **2. 규제가 추가된 경사하강법 적용**


```python
eta = 0.1
n_iterations = 5001
m = len(x_train)
epsilon = 1e-7
alpha = 0.1        # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = x_train.dot(Theta)
    y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -1/m * (np.sum(y_train_one_hot * np.log(y_proba + epsilon) + (1 - y_train_one_hot) * np.log((1 - y_proba) + epsilon)))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = y_proba - y_train_one_hot
    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * x_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients
```

    0 3.087264017788414
    500 1.2618575736554367
    1000 1.2089622673431286
    1500 1.1968382727253692
    2000 1.1927891041110146
    2500 1.1912196944095856
    3000 1.1905678513641829
    3500 1.1902870652431188
    4000 1.1901635345054586
    4500 1.190108476626593
    5000 1.1900837326785272


규제로 인해 손실이 증가 하였으나, 정확도는 0.967로 증가


```python
logits = x_valid.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.9666666666666667



## **3. 조기 종료 추가 적용**


```python
eta = 0.1 
n_iterations = 5001
m = len(x_train)
epsilon = 1e-7
alpha = 0.1            # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = x_train.dot(Theta)
    y_proba = sigmoid(logits)
    error = y_proba - y_train_one_hot
    gradients = 1/m * x_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = x_valid.dot(Theta)
    y_proba = sigmoid(logits)
    xentropy_loss = -1/m * (np.sum(y_valid_one_hot * np.log(y_proba + epsilon) + (1 - y_valid_one_hot) * np.log((1 - y_proba) + epsilon)))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break
```

    0 1.6312532353053648
    500 0.4699637566969542
    560 0.4696338340608826
    561 0.4696339699539629 조기 종료!


조기 종료를 사용 하니 검증 세트에 대한 정확도는 0.8로 낮아졌다.


```python
logits = x_valid.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```




    0.8



**전체 데이터셋에 대한 예측 결과 그래프**

등고선을 세토사, 버시컬러, 버지니카 각각의 예측 기준으로 그려보니 세 가지 형태의 등고선이 나타남.


```python
x0, x1 = np.meshgrid(
        np.linspace(0, 8, 500).reshape(-1, 1),
        np.linspace(0, 3.5, 200).reshape(-1, 1),
    )
X_new = np.c_[x0.ravel(), x1.ravel()]
X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]

# 직사각형 점 대상 예측하기
logits = X_new_with_bias.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

# 등고선용 정보
zz0 = y_proba[:, 0].reshape(x0.shape) # 세토사 기준 예측 확률
zz1 = y_proba[:, 1].reshape(x0.shape) # 버시컬러 기준 예측 확률
zz2 = y_proba[:, 2].reshape(x0.shape) # 버지니카 기준 예측 확률
zz = y_predict.reshape(x0.shape) # 예측값


# 붓꽃 샘플 그리기
plt.figure(figsize=(10, 4))
plt.plot(x[y==2, 0], x[y==2, 1], "g^", label="Iris virginica")   # 녹색 삼각형, 버지니카
plt.plot(x[y==1, 0], x[y==1, 1], "bs", label="Iris versicolor")  # 파랑 사각형, 버시컬러
plt.plot(x[y==0, 0], x[y==0, 1], "yo", label="Iris setosa")      # 노랑 원, 세토사

# 등고선 그리기
from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])

plt.contourf(x0, x1, zz, cmap=custom_cmap)                       # 노랑, 청보라, 녹색 바탕색
contour = plt.contour(x0, x1, zz0, cmap=plt.cm.brg)              # 버시컬러 기준 예측 확률 등고선
contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)              # 버시컬러 기준 예측 확률 등고선
contour = plt.contour(x0, x1, zz2, cmap=plt.cm.brg)              # 버시컬러 기준 예측 확률 등고선
plt.clabel(contour, inline=1, fontsize=12)

# 기타 도표 정보
plt.xlabel("Petal length", fontsize=14)
plt.ylabel("Petal width", fontsize=14)
plt.legend(loc="upper left", fontsize=14)
plt.axis([0, 7, 0, 3.5])
plt.show()
```


![output_90_0](https://user-images.githubusercontent.com/80394894/117976452-c2245b80-b36a-11eb-8457-c0466018b386.png)

    


테스트 세트에 대한 모델의 최종 성능의 정확도는 0.867


```python
logits = x_test.dot(Theta)
y_proba = sigmoid(logits)
y_predict = np.argmax(y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_test)
accuracy_score
```




    0.8666666666666667



#과제3 : 사진을 낮과 밤으로 분류하는 로지스틱 회귀 모델 

**낮과 밤 사진**

https://drive.google.com/file/d/1IaFZ2KkGTciS0qKP2oUWpt0kfNHaGrya/view?usp=sharing

**실내 및 실외 사진**

https://drive.google.com/file/d/1OtjLInLHiCZIAptCpMxiWZZ34jW9OpeE/view?usp=sharing

위 사진들은 직접 구글링하여 다운로드 한 자료들로 구글 드라이브로 공유한 링크입니다.


## A. 사진을 낮과 밤으로 분류하는 로지스틱 회귀 모델을 구현


*   공통 모듈 임포트
*   구글 드라이브 연동
*   경로 지정 및 데이터 불러오기



**모듈 임포트**


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2 
import os 
from random import shuffle 
from tqdm import tqdm 
from PIL import Image
import warnings
warnings.filterwarnings('ignore')
import os
```


```python
import os
import zipfile as zf
from google.colab import drive 
drive.mount('/content/drive/') # 구글 드라이브 연동
```

    Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount("/content/drive/", force_remount=True).


**이미지 불러오기**


```python
filename = '/content/drive/MyDrive/image.zip' # 구글 드라이브 내 실외 실내 이미지 압축파일 불러오기
zip_ref = zf.ZipFile(filename, 'r')

zip_ref.extractall('/tmp') # tmp 경로에 압축풀기
zip_ref.close()

np.random.seed(42) # 랜덤 시드 초기화
```

**경로 지정하기**


```python
base_dir = '/tmp/image' # 기본 경로

# 훈련 세트와 검증 세트 경로 지정
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')

# 훈련에 사용될 이미지 경로 
train_afternoon_dir = os.path.join(train_dir, 'afternoon')
train_night_dir = os.path.join(train_dir, 'night')
print(train_afternoon_dir)
print(train_night_dir)

# 테스트에 사용될 이미지 경로
validation_afternoon_dir = os.path.join(validation_dir, 'afternoon')
validation_night_dir = os.path.join(validation_dir, 'night')
print(validation_afternoon_dir)
print(validation_night_dir)
```

    /tmp/image/train/afternoon
    /tmp/image/train/night
    /tmp/image/validation/afternoon
    /tmp/image/validation/night


**데이터 확인하기**

`os.listdir()`메서드를 통해 경로 내에 있는 이미지의 파일명을 리스트 형태로 변환


```python
train_afternoon = os.listdir(train_afternoon_dir)
train_night = os.listdir(train_night_dir)
valid_afternoon = os.listdir(validation_afternoon_dir)
valid_night = os.listdir(validation_night_dir)

print(train_afternoon[:5])
print(train_night[:5])
```

    ['afternoon18.jpeg', 'afternoon38.jpeg', 'afternoon16.jpeg', 'afternoon37.jpeg', 'afternoon15.jpeg']
    ['night3.jpeg', 'night25.jpeg', 'night12.jpeg', 'night1.jpeg', 'night27.jpeg']



```python
print(len(train_afternoon)) # 훈련 세트의 낮 사진 개수
print(len(train_night))     # 훈련 세트의 밤 사진 개수

print(len(valid_afternoon)) # 검증 세트의 낮 사진 개수
print(len(valid_night))     # 검증 세트의 밤 사진 개수
```

    42
    40
    10
    10


**이미지 확인하기**

훈련에 사용될 낮과 밤의 이미지를 직접 눈으로 확인


```python
import matplotlib.image as mping
```


```python
nrows, ncols = 4, 4 # 4x4 개의 이미지 확인
pic_index = 0

fig = plt.gcf()
fig.set_size_inches(ncols*3, nrows*3) #출력 크기 설정

pic_index+=8

#낮 사진 8개
next_afternoon_pix = [os.path.join(train_afternoon_dir, fname)
                for fname in train_afternoon[ pic_index-8:pic_index]]

#밤 사진 8개
next_night_pix = [os.path.join(train_night_dir, fname)
                for fname in train_night[ pic_index-8:pic_index]]

for i, img_path in enumerate(next_afternoon_pix+next_night_pix):
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off')

  img = mping.imread(img_path)
  plt.imshow(img)

plt.show()
```


![output_108_0](https://user-images.githubusercontent.com/80394894/117976482-cbadc380-b36a-11eb-8a38-5e8c3c3609ab.png)


**이미지 크기 조정**

사진의 크기에 따라 특성이 다르게 측정될 수 있기 때문에 같은 크기의 이미지를 갖도록 조정


```python
image_size = 128
for image in tqdm(os.listdir(train_night_dir)): 
    path = os.path.join(train_night_dir, image)
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img = cv2.resize(img, (image_size, image_size)).flatten()   
    np_img=np.asarray(img)
    
for image2 in tqdm(os.listdir(train_afternoon_dir)): 
    path = os.path.join(train_afternoon_dir, image2)
    img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img2 = cv2.resize(img2, (image_size, image_size)).flatten() 
    np_img2=np.asarray(img2)

plt.figure(figsize=(10,10))
plt.subplot(1, 2, 1)
plt.imshow(np_img.reshape(image_size, image_size))
plt.axis('off')
plt.subplot(1, 2, 2)
plt.imshow(np_img2.reshape(image_size, image_size))
plt.axis('off')
plt.title("afternoon and night in GrayScale")
```

    100%|██████████| 40/40 [00:00<00:00, 1235.48it/s]
    100%|██████████| 42/42 [00:00<00:00, 801.00it/s]





    Text(0.5, 1.0, 'afternoon and night in GrayScale')




![output_110_2](https://user-images.githubusercontent.com/80394894/117976524-d9634900-b36a-11eb-845f-3713a7cc5338.png)
    


**경로에 따른 이미지 작업**

훈련 세트와 검증 세트를 하나의 훈련 세트로 합쳐준다.


```python
def train_data():
    train_data_night = [] 
    train_data_afternoon=[]
    for image1 in tqdm(os.listdir(train_night_dir)): 
        path = os.path.join(train_night_dir, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        train_data_night.append(img1) 
    for image2 in tqdm(os.listdir(train_afternoon_dir)): 
        path = os.path.join(train_afternoon_dir, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        train_data_afternoon.append(img2) 
    
    train_data= np.concatenate((np.asarray(train_data_night),np.asarray(train_data_afternoon)),axis=0)
    return train_data 
```


```python
def valid_data():
    valid_data_night = [] 
    valid_data_afternoon=[]
    for image1 in tqdm(os.listdir(validation_night_dir)): 
        path = os.path.join(validation_night_dir, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        valid_data_night.append(img1) 
    for image2 in tqdm(os.listdir(validation_afternoon_dir)): 
        path = os.path.join(validation_afternoon_dir, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        valid_data_afternoon.append(img2) 
    
    valid_data= np.concatenate((np.asarray(valid_data_night),np.asarray(valid_data_afternoon)),axis=0) 
    return valid_data 
```

훈련 세트와 검증 세트를 분리하여 저장


```python
train_data = train_data()
valid_data = valid_data()
```

    100%|██████████| 40/40 [00:00<00:00, 1658.94it/s]
    100%|██████████| 42/42 [00:00<00:00, 1325.54it/s]
    100%|██████████| 10/10 [00:00<00:00, 1689.68it/s]
    100%|██████████| 10/10 [00:00<00:00, 1262.05it/s]



```python
x_data = np.concatenate((train_data, valid_data),axis=0)
x_data = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))
```


```python
z1 = np.zeros(40)
o1 = np.ones(42)
Y_train = np.concatenate((o1, z1), axis=0)
z = np.zeros(10)
o = np.ones(10)
Y_valid = np.concatenate((o, z), axis=0)
```


```python
y_data = np.concatenate((Y_train, Y_valid),axis=0).reshape(x_data.shape[0],1)
```


```python
print("X shape: " , x_data.shape)
print("Y shape: " , y_data.shape)
```

    X shape:  (102, 128, 128)
    Y shape:  (102, 1)


**데이터 전처리**

* 사이킷런을 이용하여 훈련 세트와 검증 세트 분리


```python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=42)
number_of_train = x_train.shape[0]
number_of_test = x_test.shape[0]
```


```python
x_train_flatten = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])
x_test_flatten = x_test .reshape(number_of_test,x_test.shape[1]*x_test.shape[2])
print("X train flatten",x_train_flatten.shape)
print("X test flatten",x_test_flatten.shape)
```

    X train flatten (86, 16384)
    X test flatten (16, 16384)



```python
x_train = x_train_flatten.T
x_test = x_test_flatten.T
y_test = y_test.T
y_train = y_train.T
afternoon_night_y_test = y_test
print("x train: ",x_train.shape)
print("x test: ",x_test.shape)
print("y train: ",y_train.shape)
print("y test: ",y_test.shape)
```

    x train:  (16384, 86)
    x test:  (16384, 16)
    y train:  (1, 86)
    y test:  (1, 16)


**모델 훈련**

* 로지스틱 회귀 모델 구현 및 적용



```python
def initialize_weights_and_bias(dimension):
    w = np.full((dimension,1),0.01)
    b = 0.0
    return w, b

def sigmoid(z):
    y_head = 1/(1+np.exp(-z))
    return y_head

def forward_backward_propagation(w,b,x_train,y_train):
    # forward propagation
    z = np.dot(w.T,x_train) + b
    y_head = sigmoid(z)
    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)
    cost = (np.sum(loss))/x_train.shape[1]
    # backward propagation
    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]
    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]
    gradients = {"derivative_weight": derivative_weight,"derivative_bias": derivative_bias}
    return cost,gradients

def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):
    cost_list = []
    cost_list2 = []
    index = []
    
    for i in range(number_of_iterarion):
        
        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)
        cost_list.append(cost)
        
        w = w - learning_rate * gradients["derivative_weight"]
        b = b - learning_rate * gradients["derivative_bias"]
        if i % 100 == 0:
            cost_list2.append(cost)
            index.append(i)
            print ("Cost after iteration %i: %f" %(i, cost))
    
    parameters = {"weight": w,"bias": b}
    plt.plot(index,cost_list2)
    plt.xticks(index,rotation='vertical')
    plt.xlabel("Number of Iterarion")
    plt.ylabel("Cost")
    plt.show()
    return parameters, gradients, cost_list

def predict(w,b,x_test):
    
    z = sigmoid(np.dot(w.T,x_test)+b)
    Y_prediction = np.zeros((1,x_test.shape[1]))

    for i in range(z.shape[1]):
        if z[0,i]<= 0.5:
            Y_prediction[0,i] = 0
        else:
            Y_prediction[0,i] = 1

    return Y_prediction

def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):

    dimension =  x_train.shape[0]
    w,b = initialize_weights_and_bias(dimension)

    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)
    
    y_prediction_test = predict(parameters["weight"],parameters["bias"],x_test)
    y_prediction_train = predict(parameters["weight"],parameters["bias"],x_train)
    
    print("Test Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))
    print("Train Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))
```


```python
logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 1000)
```

    Cost after iteration 0: nan
    Cost after iteration 100: 0.245109
    Cost after iteration 200: 0.130664
    Cost after iteration 300: 0.086807
    Cost after iteration 400: 0.070968
    Cost after iteration 500: 0.059956
    Cost after iteration 600: 0.051823
    Cost after iteration 700: 0.045573
    Cost after iteration 800: 0.040628
    Cost after iteration 900: 0.036624




![output_126_1](https://user-images.githubusercontent.com/80394894/117976561-e1bb8400-b36a-11eb-8156-8709a11be9f8.png)
    


    Test Accuracy: 43.75 %
    Train Accuracy: 100.0 %


훈련 세트는 100%의 정확도를 나타냈으나,
검증 세트는 43.75%로 낮은 정확도를 보여주었고 훈련이 진행됨에 따라 성능이 떨어졌다.

# C-A. 과제에서 구현한 알고리즘과 사이킷런에서 제공하는 로지스틱 회귀 모델 성능 비교

데이터를 사이킷런의 로지스틱 회귀 훈련을 하기 위해 알맞는 형태로 데이터를 전처리 해야한다.


```python
x_train.shape
```




    (16384, 86)




```python
y_train.shape
```




    (1, 86)




```python
y_train2 = np.array([])
for i in y_train:
  y_train2 = np.append(y_train2, np.array([i]))
```


```python
y_test2 = np.array([])
for i in y_test:
  y_test2 = np.append(y_test2, np.array([i]))
```


```python
y_train2.shape
```




    (86,)



**로지스틱 회귀 훈련**


```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(penalty='none', 
                         tol=0.1, solver='saga',
                         multi_class='multinomial').fit(x_train.T, y_train2)
```


```python
clf.score(x_test.T, y_test2)
```




    0.5625




```python
pred1 = clf.predict(x_test.T)
```

직접 구현한 로지스틱 회귀 : 43.75%

사이킷 런의 로지스틱 회귀 : 56.25%

직접 구현한 것보다 사이킷 런의 로지스틱 회귀 모델의 정확도가 더욱 높았다.

# B. 낮과 밤, 실내와 실외로 분류하는 다중 레이블 분류 모델을 두 개의 로지스틱 회귀 모델을 이용하여 구현

**실외/실내 사진 불러오기**


```python
filename = '/content/drive/MyDrive/image2.zip' # 구글 드라이브 내 실외 실내 이미지 압축파일 불러오기
zip_ref2 = zf.ZipFile(filename, 'r')

zip_ref2.extractall('/tmp') # tmp 경로에 압축풀기
zip_ref2.close()

np.random.seed(42) # 랜덤 시드 초기화
```


```python
base_dir = '/tmp/image2' # 기본 경로

train_dir2 = os.path.join(base_dir, 'train')
validation_dir2 = os.path.join(base_dir, 'validation')

# 훈련에 사용될 이미지 경로 
train_indoor_dir = os.path.join(train_dir2, 'indoor')
train_outdoor_dir = os.path.join(train_dir2, 'outdoor')
print(train_indoor_dir)
print(train_outdoor_dir)

# 테스트에 사용될 이미지 경로
validation_indoor_dir = os.path.join(validation_dir2, 'indoor')
validation_outdoor_dir = os.path.join(validation_dir2, 'outdoor')
print(validation_indoor_dir)
print(validation_outdoor_dir)
```

    /tmp/image2/train/indoor
    /tmp/image2/train/outdoor
    /tmp/image2/validation/indoor
    /tmp/image2/validation/outdoor



```python
train_indoor = os.listdir(train_indoor_dir)
train_outdoor = os.listdir(train_outdoor_dir)
valid_indoor = os.listdir(validation_indoor_dir)
valid_outdoor = os.listdir(validation_outdoor_dir)

print(train_indoor[:5])
print(train_outdoor[:5])
```

    ['indoor40.jpeg', 'indoor25.jpeg', 'indoor47.jpeg', 'indoor52.jpeg', 'indoor31.jpeg']
    ['outdoor43.jpeg', 'outdoor14.jpeg', 'outdoor35.jpeg', 'outdoor26.jpeg', 'outdoor40.jpeg']


**데이터 개수**


*   훈련 세트

 낮 사진 : 42개

 밤 사진 : 40개

 실내 사진 : 50개

 실외 사진 : 49개

*   검증 세트는 각 10개씩




```python
#데이터 개수
print(len(train_afternoon)) # 훈련 세트의 낮 사진 개수
print(len(train_night))     # 훈련 세트의 밤 사진 개수
print(len(train_indoor))     # 훈련 세트의 실내 사진
print(len(train_outdoor))   # 훈련 세트의 실외 사진

print(len(valid_afternoon)) # 검증 세트의 낮 사진 개수
print(len(valid_night))     # 검증 세트의 밤 사진 개수
print(len(valid_indoor))    # 검증 세트의 실내 사진
print(len(valid_outdoor))   # 검증 세트의 실외 사진
```

    42
    40
    50
    49
    10
    10
    10
    10


**이미지 확인**


```python
nrows, ncols = 4, 4 # 4x4 개의 이미지 확인
pic_index = 0

fig = plt.gcf()
fig.set_size_inches(ncols*3, nrows*3) #출력 크기 설정

pic_index+=4

#낮 사진 4개
next_afternoon_pix = [os.path.join(train_afternoon_dir, fname)
                for fname in train_afternoon[ pic_index-4:pic_index]]

#밤 사진 4개
next_night_pix = [os.path.join(train_night_dir, fname)
                for fname in train_night[ pic_index-4:pic_index]]

#실내 사진 4개
next_indoor_pix = [os.path.join(train_indoor_dir, fname)
                for fname in train_indoor[ pic_index-4:pic_index]]

#실외 사진 4개
next_outdoor_pix = [os.path.join(train_outdoor_dir, fname)
                for fname in train_outdoor[ pic_index-4:pic_index]]

for i, img_path in enumerate(next_afternoon_pix+next_night_pix+next_indoor_pix+next_outdoor_pix):
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off')

  img = mping.imread(img_path)
  plt.imshow(img)

plt.show()
```


​    
![output_148_0](https://user-images.githubusercontent.com/80394894/117976581-e6803800-b36a-11eb-99a0-79f1c239f0a2.png)
​    


**이미지 크기 조정**

사진의 크기에 따라 특성이 다르게 측정될 수 있기 때문에 같은 크기의 이미지를 갖도록 조정


```python
image_size = 128
for image in tqdm(os.listdir(train_indoor_dir)): 
    path = os.path.join(train_indoor_dir, image)
    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img = cv2.resize(img, (image_size, image_size)).flatten()   
    np_img=np.asarray(img)
    
for image2 in tqdm(os.listdir(train_outdoor_dir)): 
    path = os.path.join(train_outdoor_dir, image2)
    img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
    img2 = cv2.resize(img2, (image_size, image_size)).flatten() 
    np_img2=np.asarray(img2)

plt.figure(figsize=(10,10))
plt.subplot(1, 2, 1)
plt.imshow(np_img.reshape(image_size, image_size))
plt.axis('off')
plt.subplot(1, 2, 2)
plt.imshow(np_img2.reshape(image_size, image_size))
plt.axis('off')
plt.title("indoor and outdoor in GrayScale")
```

    100%|██████████| 50/50 [00:00<00:00, 1329.85it/s]
    100%|██████████| 49/49 [00:00<00:00, 1267.56it/s]





    Text(0.5, 1.0, 'indoor and outdoor in GrayScale')




​    
![output_150_2](https://user-images.githubusercontent.com/80394894/117976596-eb44ec00-b36a-11eb-8950-f749d3a0bdbd.png)
​    


**경로에 따른 이미지 작업**

훈련 세트와 검증 세트를 하나의 훈련 세트로 합쳐준다.


```python
def train_data():
    train_data_indoor = [] 
    train_data_outdoor=[]
    for image1 in tqdm(os.listdir(train_indoor_dir)): 
        path = os.path.join(train_indoor_dir, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        train_data_indoor.append(img1) 
    for image2 in tqdm(os.listdir(train_outdoor_dir)): 
        path = os.path.join(train_outdoor_dir, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        train_data_outdoor.append(img2) 
    
    train_data= np.concatenate((np.asarray(train_data_indoor),np.asarray(train_data_outdoor)),axis=0)
    return train_data 
```


```python
def valid_data():
    valid_data_indoor = [] 
    valid_data_outdoor=[]
    for image1 in tqdm(os.listdir(validation_indoor_dir)): 
        path = os.path.join(validation_indoor_dir, image1)
        img1 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img1 = cv2.resize(img1, (image_size, image_size))
        valid_data_indoor.append(img1) 
    for image2 in tqdm(os.listdir(validation_outdoor_dir)): 
        path = os.path.join(validation_outdoor_dir, image2)
        img2 = cv2.imread(path, cv2.IMREAD_GRAYSCALE) 
        img2 = cv2.resize(img2, (image_size, image_size))
        valid_data_outdoor.append(img2) 
    
    valid_data= np.concatenate((np.asarray(valid_data_indoor),np.asarray(valid_data_outdoor)),axis=0)
    return valid_data 
```

훈련 세트와 검증 세트를 분리하여 저장


```python
train_data = train_data() 
valid_data = valid_data()
```

    100%|██████████| 50/50 [00:00<00:00, 1497.36it/s]
    100%|██████████| 49/49 [00:00<00:00, 1700.09it/s]
    100%|██████████| 10/10 [00:00<00:00, 1317.35it/s]
    100%|██████████| 10/10 [00:00<00:00, 1257.29it/s]



```python
x_data=np.concatenate((train_data,valid_data),axis=0)
x_data = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))
```


```python
z1 = np.zeros(50)
o1 = np.ones(49)
Y_train = np.concatenate((o1, z1), axis=0)
z = np.zeros(10)
o = np.ones(10)
Y_test = np.concatenate((o, z), axis=0)
```


```python
y_data=np.concatenate((Y_train,Y_test),axis=0).reshape(x_data.shape[0],1)
```


```python
print("X shape: " , x_data.shape)
print("Y shape: " , y_data.shape)
```

    X shape:  (119, 128, 128)
    Y shape:  (119, 1)


**데이터 전처리**

* 사이킷런을 이용하여 훈련 세트와 검증 세트 분리


```python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=42)
number_of_train = x_train.shape[0]
number_of_test = x_test.shape[0]
```


```python
x_train_flatten = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])
x_test_flatten = x_test .reshape(number_of_test,x_test.shape[1]*x_test.shape[2])
print("X train flatten",x_train_flatten.shape)
print("X test flatten",x_test_flatten.shape)
```

    X train flatten (101, 16384)
    X test flatten (18, 16384)



```python
x_train = x_train_flatten.T
x_test = x_test_flatten.T
y_test = y_test.T
y_train = y_train.T
out_doors_y_test = y_test
print("x train: ",x_train.shape)
print("x test: ",x_test.shape)
print("y train: ",y_train.shape)
print("y test: ",y_test.shape)
```

    x train:  (16384, 101)
    x test:  (16384, 18)
    y train:  (1, 101)
    y test:  (1, 18)


**모델 훈련**

* 로지스틱 회귀 모델 구현 및 적용



```python
def initialize_weights_and_bias(dimension):
    w = np.full((dimension,1),0.01)
    b = 0.0
    return w, b

def sigmoid(z):
    y_head = 1/(1+np.exp(-z))
    return y_head

def forward_backward_propagation(w,b,x_train,y_train):
    # forward propagation
    z = np.dot(w.T,x_train) + b
    y_head = sigmoid(z)
    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)
    cost = (np.sum(loss))/x_train.shape[1]
    # backward propagation
    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]
    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]
    gradients = {"derivative_weight": derivative_weight,"derivative_bias": derivative_bias}
    return cost,gradients

def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):
    cost_list = []
    cost_list2 = []
    index = []
    
    for i in range(number_of_iterarion):
        
        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)
        cost_list.append(cost)
        
        w = w - learning_rate * gradients["derivative_weight"]
        b = b - learning_rate * gradients["derivative_bias"]
        if i % 100 == 0:
            cost_list2.append(cost)
            index.append(i)
            print ("Cost after iteration %i: %f" %(i, cost))
    
    parameters = {"weight": w,"bias": b}
    plt.plot(index,cost_list2)
    plt.xticks(index,rotation='vertical')
    plt.xlabel("Number of Iterarion")
    plt.ylabel("Cost")
    plt.show()
    return parameters, gradients, cost_list

def predict(w,b,x_test):
    
    z = sigmoid(np.dot(w.T,x_test)+b)
    Y_prediction = np.zeros((1,x_test.shape[1]))

    for i in range(z.shape[1]):
        if z[0,i]<= 0.5:
            Y_prediction[0,i] = 0
        else:
            Y_prediction[0,i] = 1

    return Y_prediction

def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):

    dimension =  x_train.shape[0]
    w,b = initialize_weights_and_bias(dimension)

    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)
    
    y_prediction_test = predict(parameters["weight"],parameters["bias"],x_test)
    y_prediction_train = predict(parameters["weight"],parameters["bias"],x_train)
    
    print("Test Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))
    print("Train Accuracy: {} %".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))
```


```python
logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 1000)
```

    Cost after iteration 0: nan
    Cost after iteration 100: 4.705022
    Cost after iteration 200: 2.903181
    Cost after iteration 300: 0.364168
    Cost after iteration 400: 0.040100
    Cost after iteration 500: 0.027468
    Cost after iteration 600: 0.022541
    Cost after iteration 700: 0.019471
    Cost after iteration 800: 0.017319
    Cost after iteration 900: 0.015705




![output_150_2](https://user-images.githubusercontent.com/80394894/117976612-f009a000-b36a-11eb-83ba-53756ce2b4da.png)
    


    Test Accuracy: 55.56 %
    Train Accuracy: 100.0 %


훈련 세트의 정확도는 100%가 나왔지만, 검증세트는 56.56%로 낮은 정확도가 나타났다.

또한 훈련이 진행될 수록 성능은 급격히 저하되었다.

# C-B. 과제에서 구현한 알고리즘과 사이킷런에서 제공하는 로지스틱 회귀 모델 성능 비교


```python
in_out_y_train = np.array([])
for i in y_train:
  in_out_y_train = np.append(in_out_y_train, np.array([i]))
```


```python
in_out_y_test = np.array([])
for i in y_test:
  in_out_y_test = np.append(in_out_y_test, np.array([i]))
```


```python
in_out_y_test
```




    array([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,
           1.])




```python
from sklearn.linear_model import LogisticRegression

lg2 = LogisticRegression(penalty='none', 
                         tol=0.1, solver='saga',C = 0.5,
                         multi_class='multinomial').fit(x_train.T, in_out_y_train)
```


```python
pred2 = lg2.predict(x_test.T)
```


```python
lg2.score(x_test.T, in_out_y_test)
```




    0.7222222222222222



직접 구현한 로지스틱 회귀 : 55.56%

사이킷 런의 로지스틱 회귀 : 72.22%

직접 구현한 것보다 사이킷 런의 로지스틱 회귀 모델의 정확도가 더욱 높았다.

두 가지의 모델의 예측값을 하나로 합쳐 다중 레이블 분류를 만든 후 정확도 성능 측정


```python
multi_label_list = []
for i in range(len(pred1)):
  multi_label_list.append([pred1[i], pred2[i]])
```


```python
multi_label_pred = np.array(multi_label_list)
```


```python
multi_label_test_list = []
for i in range(len(out_doors_y_test)):
 multi_label_test_list.append([afternoon_night_y_test[0][i], out_doors_y_test[0][i]])
```


```python
multi_label_y_test = np.array(multi_label_test_list)
```


```python
accuracy_score = np.mean(multi_label_pred == multi_label_y_test)
accuracy_score
```




    0.46875



다중 레이블 분류 모델의 성능은 46.875%의 정확도가 나타났다.
