---
layout: post
title:  "[ML] 차원축소 알고리즘 성능 비교"
---

# [ML] 차원축소 알고리즘 성능 비교

**차원축소 알고리즘 성능 비교**
1.   데이터셋 불러온 후 훈련, 테스트 세트 분할하기(MNIST 데이터셋 사용)
2.   다양한 차원축소 알고리즘 적용하여 2차원으로 차원 축소 하기
3.   변환된 데이터셋에 대해 SVC, 랜덤포레스트 등의 분류기 학습
4.   각 분류기의 성능 평가

추가:   3차원으로 차원 축소 후 진행한 결과와 성능 비교




**공통 모듈 임포트하기**


```python
# 파이썬 ≥3.7 필수
import sys
assert sys.version_info >= (3, 7)

# 사이킷런 ≥0.20 필수
import sklearn
assert sklearn.__version__ >= "0.20"

# 공통 모듈 임포트
import numpy as np
import os

# 노트북 실행 결과를 동일하게 유지하기 위해
np.random.seed(42)

# 깔끔한 그래프 출력을 위해
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# 그림을 저장할 위치
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "dim_reduction"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("그림 저장", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
```



## 데이터셋 불러온 후 훈련, 테스트 세트로 분할하기(MNIST 데이터셋 사용)

**MNIST 데이터를 불러온 후 훈련 세트와 테스트 세트로 분할합니다.**


```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version=1, as_frame=False)
mnist.target = mnist.target.astype(np.uint8)
```



훈련 세트와 테스트 세트를 분할하기 전에 원할한 속도로 차원 축소를 진행하기 위해 5000개의 이미지 데이터셋으로 대폭 축소하여 진행하겠습니다.

참고 : 컴퓨터 사양이 좋은 경우 10000개 이상의 데이터셋으로 진행해보셔도 됩니다.


```python
np.random.seed(42)

m = 5000
idx = np.random.permutation(60000)[:m]

X = mnist['data'][idx]
y = mnist['target'][idx]
```



**이후 4000개의 훈련 세트와 1000개의 테스트 세트로 분할합니다.**


```python
X_train = X[:4000]
y_train = y[:4000]

X_test = X[4000:]
y_test = y[4000:]
```



훈련세트와 테스트 세트로 잘 분할되었는지 확인합니다.


```python
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
```

    [Out]
    (4000, 784)
    (4000,)
    (1000, 784)
    (1000,)



**시각화 그래프**

`plot_digits()`함수를 만들어 축소된 데이터를 산점도와 색깔있는 숫자로 시각하기 위해 사용합니다.


```python
from sklearn.preprocessing import MinMaxScaler
from matplotlib.offsetbox import AnnotationBbox, OffsetImage

def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):
    # 입력 특성의 스케일을 0에서 1 사이로 만듭니다.
    X_normalized = MinMaxScaler().fit_transform(X)
    # 그릴 숫자의 좌표 목록을 만듭니다.
    # 반복문 아래에서 `if` 문장을 쓰지 않기 위해 시작할 때 이미 그래프가 그려져 있다고 가정합니다.
    neighbors = np.array([[10., 10.]])
    # 나머지는 이해하기 쉽습니다.
    plt.figure(figsize=figsize)
    cmap = mpl.cm.get_cmap("jet")
    digits = np.unique(y)
    for digit in digits:
        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)])
    plt.axis("off")
    ax = plt.gcf().gca()  # 현재 그래프의 축을 가져옵니다.
    for index, image_coord in enumerate(X_normalized):
        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()
        if closest_distance > min_distance:
            neighbors = np.r_[neighbors, [image_coord]]
            if images is None:
                plt.text(image_coord[0], image_coord[1], str(int(y[index])),
                         color=cmap(y[index] / 9), fontdict={"weight": "bold", "size": 16})
            else:
                image = images[index].reshape(28, 28)
                imagebox = AnnotationBbox(OffsetImage(image, cmap="binary"), image_coord)
                ax.add_artist(imagebox)
```



## 다양한 차원축소 알고리즘 적용하여 2차원으로 차원 축소 하기

**다양한 차원 축소 알고리즘을 통해 차원축소 및 시각화를 통해 확인합니다**

차원축소 알고리즘은 아래와 같이 7가지 차원축소 알고리즘을 사용하였습니다.

* t-SNE
* PCA(주성분 분석)
* LLE(지역 선형 임베딩)
* LDA(선형 판별 분석)
* 랜덤 PCA
* 점진적 PCA
* 랜덤 투영(random projection)

위 모든 차원축소는 2차원으로 진행하기 위해 `n_components` 값은 2를 입력하였습니다.

**1. t-SNE**


```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
X_tsne_reduced = tsne.fit_transform(X_train)
```



t-SNE를 차원 축소 한 후 그래프를 살펴보니 어느정도 시간이 걸린 만큼 나름 잘 군집화되어 있습니다.


```python
plot_digits(X_tsne_reduced, y_train)
```


![output_19_0](https://user-images.githubusercontent.com/80394894/121832777-d6f76480-cd05-11eb-8c8a-b169a302baab.png)
    



**2. PCA**


```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2, random_state=42)
X_pca_reduced = pca.fit_transform(X_train)
```



일반 PCA 알고리즘의 경우 매우 빠른 시간 안에 차원 축소를 하였으나 그래프 상의 숫자들이 많이 겹쳐서 나타나고 있습니다.


```python
plot_digits(X_pca_reduced, y_train)
```


![output_23_0](https://user-images.githubusercontent.com/80394894/121832852-04dca900-cd06-11eb-8df3-04b89dfdc98b.png)



**3. LLE**


```python
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, random_state=42)
X_lle_reduced = lle.fit_transform(X_train)
```



LLE의 경우 선형적인 군집화와 어느정도 군집된 부분들이나타났으나 결과 자체는 그리 좋아보이지 않습니다.


```python
plot_digits(X_lle_reduced, y_train)
```


![output_27_0](https://user-images.githubusercontent.com/80394894/121832860-08703000-cd06-11eb-884e-4cac9b3e69b7.png)



**4. LDA**


```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=2)
X_lda_reduced = lda.fit_transform(X_train, y_train)
```



LDA의 경우 t-SNE보다는 좋은 결과는 아니지만 그나마 괜찮게 군집화가 된 모습입니다.


```python
plot_digits(X_lda_reduced, y_train)
```


![output_31_0](https://user-images.githubusercontent.com/80394894/121832867-0c03b700-cd06-11eb-817e-c23791584e4e.png)



**5. 랜덤 PCA**


```python
rndpca = PCA(n_components=2, svd_solver="randomized")
X_rndpca_reduced = rndpca.fit_transform(X_train)
```



랜덤 pca는 일반 pca와 매우 유사한 결과를 나타냈으며, 결과도 좋지 못합니다.


```python
plot_digits(X_rndpca_reduced, y_train)
```

![output_35_0](https://user-images.githubusercontent.com/80394894/121832874-11610180-cd06-11eb-811e-3ab08203373b.png)



**6. 점진적 PCA**


```python
from sklearn.decomposition import IncrementalPCA

n_batches = 10
incpca = IncrementalPCA(n_components=2)
for X_batch in np.array_split(X_train, n_batches):
  incpca.partial_fit(X_batch)

X_incpca_reduced = incpca.fit_transform(X_train)
```



점진적 PCA는 일반 주성분 분석한 결과에서 상하로 반전된 결과로 나타났습니다. 마찬가지로 결과는 그리 좋아보이지 않습니다.


```python
plot_digits(X_incpca_reduced, y_train)
```


![output_39_0](https://user-images.githubusercontent.com/80394894/121832875-132ac500-cd06-11eb-8bc8-41cdef1f1dcc.png)



**7. 랜덤 투영(Random Projection)**


```python
from sklearn.random_projection import GaussianRandomProjection

rndproj = GaussianRandomProjection(n_components=2, eps=0.1, random_state=42)
X_rndproj_reduced = rndproj.fit_transform(X_train, y_train)
```



랜덤 투영의 시각화는 가장 좋지 못한 결과로 나타났습니다.


```python
plot_digits(X_rndproj_reduced, y_train)
```


![output_43_0](https://user-images.githubusercontent.com/80394894/121832881-158d1f00-cd06-11eb-94ac-cb9855a4db2a.png)




## 변환된 데이터셋에 대해 SVC, 랜덤 포레스트 등의 분류기 학습

이제 차원축소를 진행한 데이터셋에 SVC과, 랜덤 포레스트를 모델을 사용하여 학습시킨 후 분류 해보도록 하곘습니다.

**분류기 학습 및 성능 평가 하기 전 알림**
1. LinearSVC와 랜덤 포레스트 알고리즘을 사용하여 모델 훈련을 하였습니다.
2. 하이퍼파라미터의 경우 가장 튜닝 없이 가장 기본적으로 진행하였습니다.
3. 이후 각 차원 축소된 데이터를 각 분류기에 학습 시킵니다.
4. 학습된 모델을 평가하기 위해 분할된 테스트 세트도 각 사용된 차원 축소 알고리즘에 축소 시킵니다.
5. 이후 예측기를 통해 테스트 세트에 대한 예측 후 성능을 평가합니다.


```python
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

svc_clf = LinearSVC(C=1, loss="hinge", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
```



## 분류기의 성능 평가

**1. t-SNE**

각각 선형 svc 모델과 랜덤 포레스트 모델에 t-SNE 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다.


```python
svc_clf.fit(X_tsne_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_tsne_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



테스트 세트도 차원 축소를 거친 뒤 `predict()` 메서드를 통해 각 테스트 세트를 예측하여 결과를 나타냅니다.

그 결과

svc는 0.108(10.8%),

랜덤 포레스트는 0.092(9.2%)

svc가 더 좋은 결과를 나타냈으나, 성능이 매우 안좋은 것으로 나타납니다.


```python
X_test_reduced = tsne.fit_transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.108
    rnd 분류기 : 0.092



**2. PCA(주성분 분석)**

각각 선형 svc 모델과 랜덤 포레스트 모델에 PCA 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다.


```python
svc_clf.fit(X_pca_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_pca_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



t-SNE와 같이 마찬가지로 테스트 세트도 차원 축소를 거친 뒤 `predict()` 메서드를 통해 각 테스트 세트를 예측하여 결과를 나타냅니다.

그 결과

svc는 0.124(12.4%),

랜덤 포레스트는 0.42(42%)

랜덤 포레스트가 선형 svc보다 더 좋은 결과를 나타냈으며 t-SNE를 통해 축소된 데이터 보다 더 좋은 성능을 나타내는 놀라운 결과를 보여주었습니다.


```python
X_test_reduced = pca.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.124
    rnd 분류기 : 0.42



**3. LLE**

각각 선형 svc 모델과 랜덤 포레스트 모델에 LLE 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다.


```python
svc_clf.fit(X_lle_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_lle_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



LLE 알고리즘을 통해 축소되고 선형 SVC와 랜덤 포레스트를 통해 분류된 결과

선형 SVC : 0.282(28.2%)

랜덤 포레스트 : 0.753(75.3%)

선형 SVC보다 랜덤 포레스트의 분류 결과가 더 좋게 나왔으며, LLE 알고리즘을 통해 축소된 데이터는 지금까지 사용된 차원축소 알고리즘인 t-SNE와 주성분 분석보다 더 좋은 결과를 나타내고 있습니다.


```python
X_test_reduced = lle.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.282
    rnd 분류기 : 0.753



**4. LDA**

각각 선형 svc 모델과 랜덤 포레스트 모델에 LDA 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다.


```python
svc_clf.fit(X_lda_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_lda_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



LDA 알고리즘을 통해 축소되고, 선형 SVC와 랜덤 포레스트 분류기를 통해 분류된 결과

선형 SVC : 0.325(32.5%)

랜덤 포레스트 : 0.484(48.4%)

로 나타났으며, 랜덤 포레스트가 선형 SVC보다 좋은 결과를 나타냈습니다.


```python
X_test_reduced = lda.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.325
    rnd 분류기 : 0.484



**5. 랜덤 PCA**

각각 선형 svc 모델과 랜덤 포레스트 모델에 랜덤 PCA 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다.


```python
svc_clf.fit(X_rndpca_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_rndpca_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



랜덤 PCA의 결과는

선형 SVC : 0.187(18.7%)

랜덤 포레스트 : 0.424(42.4%)

랜덤 포레스트가 선형 SVC보다 더 나은 결과를 나타내고 있으며 위에서 보여준 주성분 분석과 비교하면 선형 SVC, 랜덤 포레스트 모두 더 나은 결과를 나타내고 있습니다. 하지만 성능 차이가 그리 많이 나지는 않습니다.


```python
X_test_reduced = rndpca.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.187
    rnd 분류기 : 0.424


**6. 점진적 PCA**

각각 선형 svc 모델과 랜덤 포레스트 모델에 점진적 PCA 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다.


```python
svc_clf.fit(X_incpca_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_incpca_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



점진적 PCA의 결과는

선형 SVC : 0.188(18.8%)

랜덤 포레스트 : 0.425(42.5%)

랜덤 포레스트가 선형 SVC보다 더 나은 결과를 나타내고 있으며 위에서 보여준 주성분 분석, 랜덤 PCA 비교하면 선형 SVC, 랜덤 포레스트 모두 가장 좋은 결과를 나타내고 있음을 발견하였습니다.


```python
X_test_reduced = incpca.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.188
    rnd 분류기 : 0.425



**7. 랜덤 투영**

각각 선형 svc 모델과 랜덤 포레스트 모델에 랜덤 투영 알고리즘을 통해 축소된 훈련 세트를 `fit()` 메서드를 통해 훈련합니다


```python
svc_clf.fit(X_rndproj_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_rndproj_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



랜덤 투영의 결과는

선형 SVC : 0.181(18.1%)

랜덤 포레스트 : 0.285(28.5%)

랜덤 포레스트가 선형 SVC보다 좋은 성능을 나타내고 있습니다.

지금까지 사용한 알고리즘과 비교하면 좋지 못한 성능을 나타내고 있습니다.


```python
X_test_reduced = rndproj.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.181
    rnd 분류기 : 0.285



## 추가:   3차원으로 차원 축소 후 진행한 결과와 성능 비교

**3차원으로 축소된 데이터를 시각화 하기 위해 기존 `plot_digits()` 함수를 살짝 변형하여 새로 `plot_digits3()` 함수를 만듭니다.**

숫자 사이의 거리를 나타내는 넘파이 어레이에 1차원 추가하고, scatter 함수에 z축을 나타내는 값을 입력합니다.


```python
from sklearn.preprocessing import MinMaxScaler
from matplotlib.offsetbox import AnnotationBbox, OffsetImage

def plot_digits3(X, y, min_distance=0.05, images=None, figsize=(13, 10)):
    # 입력 특성의 스케일을 0에서 1 사이로 만듭니다.
    X_normalized = MinMaxScaler().fit_transform(X)
    # 그릴 숫자의 좌표 목록을 만듭니다.
    # 반복문 아래에서 `if` 문장을 쓰지 않기 위해 시작할 때 이미 그래프가 그려져 있다고 가정합니다.
    neighbors = np.array([[10., 10., 10]])
    # 나머지는 이해하기 쉽습니다.
    plt.figure(figsize=figsize)
    cmap = mpl.cm.get_cmap("jet")
    digits = np.unique(y)
    for digit in digits:
        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], X_normalized[y == digit, 2], c=[cmap(digit / 9)])
    plt.axis("off")
    ax = plt.gcf().gca()  # 현재 그래프의 축을 가져옵니다.
    for index, image_coord in enumerate(X_normalized):
        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()
        if closest_distance > min_distance:
            neighbors = np.r_[neighbors, [image_coord]]
            if images is None:
                plt.text(image_coord[0], image_coord[1], str(int(y[index])),
                         color=cmap(y[index] / 9), fontdict={"weight": "bold", "size": 16})
            else:
                image = images[index].reshape(28, 28)
                imagebox = AnnotationBbox(OffsetImage(image, cmap="binary"), image_coord)
                ax.add_artist(imagebox)
```



**1. t-SNE**


```python
from sklearn.manifold import TSNE

tsne3 = TSNE(n_components=3, random_state=42)
X_tsne3_reduced = tsne3.fit_transform(X_train)
```



시각화 그래프는 2차원으로 축소한 데이터와는 많이 다른 모습을 나타내고 있습니다. 또한 숫자와 숫자 사이의 거리가 늘어났습니다.


```python
plot_digits3(X_tsne3_reduced, y_train)
```


![output_88_0](https://user-images.githubusercontent.com/80394894/121832883-16be4c00-cd06-11eb-8b40-7f644529c033.png)




**2. 주성분 분석**


```python
from sklearn.decomposition import PCA

pca3 = PCA(n_components=3, random_state=42)
X_pca3_reduced = pca3.fit_transform(X_train)
```



주성분 분석의 경우 2차원 그래프와 크게 달라보이지 않으며 숫자간의 거리만 늘어난 것으로 나타났습니다.


```python
plot_digits3(X_pca3_reduced, y_train)
```


![output_92_0](https://user-images.githubusercontent.com/80394894/121832886-18880f80-cd06-11eb-8802-a4e9e3e48456.png)



**3. LLE**


```python
from sklearn.manifold import LocallyLinearEmbedding

lle3 = LocallyLinearEmbedding(n_components=3, random_state=42)
X_lle3_reduced = lle3.fit_transform(X_train)
```



LLE의 경우 2차원으로 차원 축소한 데이터와 매우 유사하게 나타나고 있습니다.


```python
plot_digits3(X_lle3_reduced, y_train)
```


![output_96_0](https://user-images.githubusercontent.com/80394894/121832890-1a51d300-cd06-11eb-8ec9-ffd0b9c5978f.png)



**4. LDA**


```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda3 = LinearDiscriminantAnalysis(n_components=3)
X_lda3_reduced = lda3.fit_transform(X_train, y_train)
```



LDA 또한 비슷하나 숫자와 숫자 사이의 거리가 다른 산점도에 비해 더 멀어진 느낌으로 나타났습니다.


```python
plot_digits3(X_lda3_reduced, y_train)
```


![output_100_0](https://user-images.githubusercontent.com/80394894/121832892-1c1b9680-cd06-11eb-8e3d-9219b9fdf0df.png)


**5. 랜덤 PCA**


```python
rndpca3 = PCA(n_components=3, svd_solver="randomized")
X_rndpca3_reduced = rndpca3.fit_transform(X_train)
```



랜덤 PCA 또한 마찬가지로 2차원으로 축소한 랜덤 PCA와, 주성분 분석과 크게 다르지 않는 모습입니다.


```python
plot_digits3(X_rndpca3_reduced, y_train)
```


![output_104_0](https://user-images.githubusercontent.com/80394894/121832894-1de55a00-cd06-11eb-9d1a-f29e07a69ec4.png)



**6. 점진적 PCA**


```python
from sklearn.decomposition import IncrementalPCA

n_batches = 10
incpca3 = IncrementalPCA(n_components=3)
for X_batch in np.array_split(X_train, n_batches):
  incpca3.partial_fit(X_batch)

X_incpca3_reduced = incpca3.fit_transform(X_train)
```



점진적 PCA의 축소 결과도 마찬가지로 주성분 분석 결과를 상하 반전된 모습처럼 나타나며 2차원 점진적 PCA 결과와는 크게 다르지 않는 것으로 나타납니다.


```python
plot_digits3(X_incpca3_reduced, y_train)
```


![output_108_0](https://user-images.githubusercontent.com/80394894/121832901-1faf1d80-cd06-11eb-8a41-ef968f16c315.png)
    



**7. 랜덤 투영**


```python
from sklearn.random_projection import GaussianRandomProjection

rndproj3 = GaussianRandomProjection(n_components=3, eps=0.1, random_state=42)
X_rndproj3_reduced = rndproj3.fit_transform(X_train, y_train)
```



랜덤 투영 역시 숫자간의 거리는 많이 벌어 졌으나, 군집화된 모습은 크게 다르지 않습니다.


```python
plot_digits3(X_rndproj3_reduced, y_train)
```


![output_112_0](https://user-images.githubusercontent.com/80394894/121832905-2178e100-cd06-11eb-855b-c7d8717f2f4b.png)


## 변환된 데이터셋에 대해 SVC, 랜덤포레스트 등의 분류기 학습

3차원으로 차원 축소한 데이터 역시 2차원과 같은 과정으로 분류기를 훈련하고, 예측결과를 나타내도록 하겠습니다.

하이퍼파라미터 역시 똑같은 모델로 구성하였습니다.


```python
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

svc_clf = LinearSVC(C=1, loss="hinge", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
```



## 각 분류기의 성능 평가

2차원으로 차원축소한 모델과 같은 과정으로 각 차원 축소 알고리즘과 결과만 나타내도록 하겠습니다.

**1. t-SNE**


```python
svc_clf.fit(X_tsne3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_tsne3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SVC : 0.186(18.6%)

랜덤 포레스트 : 0.096(9.6%)


```python
X_test_reduced = tsne3.fit_transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.186
    rnd 분류기 : 0.096



**2. 주성분 분석**


```python
svc_clf.fit(X_pca3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_pca3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SVC : 0.16(16%)

랜덤 포레스트 : 0.473(47.3%)


```python
X_test_reduced = pca3.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.16
    rnd 분류기 : 0.473



**3. LLE**


```python
svc_clf.fit(X_lle3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_lle3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SVC : 0.41(41%)

랜덤 포레스트 : 0.829(82.9%)


```python
X_test_reduced = lle3.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.41
    rnd 분류기 : 0.829



**4. LDA**


```python
svc_clf.fit(X_lda3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_lda3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SVC : 0.55(55%)

랜덤 포레스트 : 0.68(68%)


```python
X_test_reduced = lda3.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.55
    rnd 분류기 : 0.68



**5. 랜덤 PCA**


```python
svc_clf.fit(X_rndpca3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_rndpca3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SCV : 0.159(15.9%)

랜덤 포레스트 : 0.473(47.3%)


```python
X_test_reduced = rndpca3.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.159
    rnd 분류기 : 0.473



**6. 점진적 PCA**


```python
svc_clf.fit(X_incpca3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_incpca3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SVC : 0.237(23.7%)

랜덤 포레스트 : 0.478(47.8%)


```python
X_test_reduced = incpca3.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.237
    rnd 분류기 : 0.478



**7. 랜덤 투영**


```python
svc_clf.fit(X_rndproj3_reduced, y_train)
```

    [Out]
    /usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
      LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,
              intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',
              penalty='l2', random_state=42, tol=0.0001, verbose=0)




```python
rnd_clf.fit(X_rndproj3_reduced, y_train)
```


    [Out]
    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=None, oob_score=False, random_state=42, verbose=0,
                           warm_start=False)



선형 SVC : 0.221(22.1%)

랜덤 포레스트 : 0.379(37.9%)


```python
X_test_reduced = rndproj3.transform(X_test)

y_pred = svc_clf.predict(X_test_reduced)
print("svc 분류기 : {}".format(accuracy_score(y_test, y_pred)))
y_pred = rnd_clf.predict(X_test_reduced)
print("rnd 분류기 : {}".format(accuracy_score(y_test, y_pred)))
```

    [Out]
    svc 분류기 : 0.221
    rnd 분류기 : 0.379



## 최종 결과 및 성능 분석

### 차원축소 결과
```
1. t-sne 
        선형 SVC / 랜덤 포레스트
2차원     0.108  /  0.092
3차원     0.186  /  0.096

2. 주성분 분석
        선형 SVC / 랜덤 포레스트
2차원     0.124  /  0.42
3차원     0.16   /  0.473

3. LLE
        선형 SVC / 랜덤 포레스트
2차원     0.282  /  0.753
3차원     0.41   /  0.829

4. LDA
        선형 SVC / 랜덤 포레스트
2차원     0.325  /  0.484
3차원     0.55   /  0.68

5. 랜덤 PCA
        선형 SVC / 랜덤 포레스트
2차원     0.187  /  0.424
3차원     0.159  /  0.473

6. 점진적 PCA
        선형 SVC / 랜덤 포레스트
2차원     0.188  /  0.425
3차원     0.237   /  0.478

7. 랜덤 투영
        선형 SVC / 랜덤 포레스트
2차원     0.181  /  0.285
3차원     0.121  /  0.379

```



**2차원에서 가장 좋은 성능을 보인 분류기**

선형 SVC : LDA의 차원축소 0.325

랜덤 포레스트 : LLE의 차원축소 0.753




**3차원에서 가장 좋은 성능을 보인 분류기**

선형 SVC : LDA의 차원축소 0.55

랜덤 포레스트 : LLE의 차원축소 0.829



**2차원 및 3차원 차원축소 분류 결과 성능 순서**

**2차원**

선형 SVC : 
LDA > LLE > 점진적 PCA >  랜덤 PCA > 주성분 분석 > 랜덤 투영 > t-SNE

랜덤 포레스트 : LLE > LDA > 점진적 PCA > 랜덤 PCA > 주성분 분석 > 랜덤 투영 > t-SNE

**3차원**

선형 SVC : LDA > LLE > 점진적 PCA > t-SNE > 주성분 분석 > 랜덤 투영 > 랜덤 PCA

랜덤 포레스트 : LLE > LDA > 점진적 PCA > 랜덤 PCA = 주성분 분석 > 랜덤 투영 > t-SNE



**알게 된 점**

1. 일반화하기 어렵지만 위 결과로 보아 2차원 차원축소보다 3차원 차원축소의 분류 성능이 더 좋게 나타났습니다.
2. 데이터셋이 전체적으로 축소된 상태에서 훈련했기 때문에 만족스러운 성능을 발휘하지 못했습니다.
3. 훈련 세트 및 테스트 세트 모두 차원 축소된 상태에서 훈련했기 때문에 성능이 좋지 못할 수 밖에 없는것 같습니다.
4. 가장 좋은 결과로 나타날 것을 예측한 t-SNE의 결과가 좋지 않게 나왔으며, 이는 다른 방법으로 성능 예측을 해볼 필요가 있습니다.



**훈련 세트 및 테스트 세트 모두 차원 축소한 데이터를 가지고 분류를 해보았고 그 성능들을 비교하였습니다.**

**다음엔 축소후 역변환 하여 분류 성능을 비교해보는 것도 좋을 것 같습니다.**
